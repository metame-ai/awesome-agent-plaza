# LLM Safety

- [LLM Safety](#llm-safety)
  - [LLM Safety](#llm-safety-1)


## LLM Safety
- ðŸŒŸ [Introducing gpt-oss-safeguard](https://openai.com/index/introducing-gpt-oss-safeguard/) 
- **Poisoning Attacks on LLMs Require a Near-constant Number of Poison
  Samples**, `arXiv, 2510.07192`, [arxiv](http://arxiv.org/abs/2510.07192v1), [pdf](http://arxiv.org/pdf/2510.07192v1.pdf), cication: [**-1**](None) 

	 *Alexandra Souly, Javier Rando, Ed Chapman, ..., Yarin Gal, Robert Kirk*