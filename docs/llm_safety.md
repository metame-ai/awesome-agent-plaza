# LLM Safety

- [LLM Safety](#llm-safety) 
  - [Survey](#survey)
  - [LLM Safety](#llm-safety-1)
  - [Toolkits](#toolkits)


## Survey


## LLM Safety

- [**heretic)- ðŸŒŸ [Introducing gpt-oss-safeguard](https:**](https://github.com/p-e-w/heretic)- ðŸŒŸ [Introducing gpt-oss-safeguard](https://openai.com/index/introducing-gpt-oss-safeguard/) - p-e-w ![Star](https://img.shields.io/github/stars/p-e-w/heretic)- ðŸŒŸ [Introducing gpt-oss-safeguard](https:.svg?style=social&label=Star)

	 *Fully automatic censorship removal for language models](https://github.com/p-e-w/heretic)- ðŸŒŸ [Introducing gpt-oss-safeguard*
- **Poisoning Attacks on LLMs Require a Near-constant Number of Poison 
  Samples**, `arXiv, 2510.07192`, [arxiv](http://arxiv.org/abs/2510.07192v1), [pdf](http://arxiv.org/pdf/2510.07192v1.pdf), cication: [**-1**](None) 

	 *Alexandra Souly, Javier Rando, Ed Chapman, ..., Yarin Gal, Robert Kirk*

  
## Toolkits