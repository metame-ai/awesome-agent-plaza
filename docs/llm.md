# LLM Basics

- [LLM Basics](#llm-basics) 
	- [LLM Insight](#llm-insight)
	- [LLM Hallucination](#llm-hallucination)
	- [LLM Omni](#llm-omni)
	- [LLM Tutorial](#llm-tutorial)


## LLM Insight

- [Emergent Introspective Awareness in Large Language Models            Authors      Affiliations                    Published              Not published yet.              DOI              No DOI yet.       d-byline { display: none; } .d-byline-container { border-top: 1px solid rgba(0,0,0,0.1); border-bottom: 1px solid rgba(0,0,0,0.1); padding-top: 15px; padding-bottom: 15px; } .d-byline { grid-column: text; display: grid; grid-template-columns: [authors info-start] 4fr [affiliations published] 2fr [info-end]; grid-template-rows: [authors-start published-start] auto [published-end affiliations-start] auto [affiliations-end] auto [authors-end info] auto; font-size: 0.8rem; line-height: 1.8em; grid-gap: 5px; } .d-byline h3 { font-size: 0.6rem; font-weight: 400; color: rgba(0, 0, 0, 0.5); margin: 0; text-transform: uppercase; margin-bottom: 4px; } .d-byline .info { color: rgba(0, 0, 0, 0.5); font-size: 80%; line-height: 1.5; } .d-byline .authors div { line-height: 1.5; } .d-byline .authors sup { font-size: 80%; margin-right: -4px; } .d-byline .author { margin-right: 6px; white-space:nowrap; } .d-byline a { color: inherit; text-decoration: none; }](https://transformer-circuits.pub/2025/introspection/index.html)- [Signs of introspection in large language models](https://www.anthropic.com/research/introspection) 
- **Verifying Chain-of-Thought Reasoning via Its Computational Graph**, `arXiv, 2510.09312`, [arxiv](http://arxiv.org/abs/2510.09312v1), [pdf](http://arxiv.org/pdf/2510.09312v1.pdf), cication: [**-1**](None) 

	 *Zheng Zhao, Yeskendir Koishekenov, Xianjun Yang, ..., Naila Murray, Nicola Cancedda*
- ðŸŒŸ **The Dragon Hatchling: The Missing Link between the Transformer and 
  Models of the Brain**, `arXiv, 2509.26507`, [arxiv](http://arxiv.org/abs/2509.26507v1), [pdf](http://arxiv.org/pdf/2509.26507v1.pdf), cication: [**-1**](None) 

	 *Adrian Kosowski, PrzemysÅ‚aw UznaÅ„ski, Jan Chorowski, ..., Zuzanna Stamirowska, MichaÅ‚ Bartoszkiewicz*


## LLM Hallucination
- **Learning to Reason for Hallucination Span Detection**, `arXiv, 2510.02173`, [arxiv](http://arxiv.org/abs/2510.02173v2), [pdf](http://arxiv.org/pdf/2510.02173v2.pdf), cication: [**-1**](None) 

	 *Hsuan Su, Ting-Yao Hu, Hema Swetha Koppula, ..., Oncel Tuzel, Raviteja Vemulapalli*
- **When Models Lie, We Learn: Multilingual Span-Level Hallucination 
  Detection with PsiloQA**, `arXiv, 2510.04849`, [arxiv](http://arxiv.org/abs/2510.04849v1), [pdf](http://arxiv.org/pdf/2510.04849v1.pdf), cication: [**-1**](None) 

	 *Elisei Rykov, Kseniia Petrushina, Maksim Savkin, ..., Vasily Konovalov, Julia Belikova*


## LLM Omni
- **Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal 
  Perception and Generation**, `arXiv, 2510.24821`, [arxiv](http://arxiv.org/abs/2510.24821v1), [pdf](http://arxiv.org/pdf/2510.24821v1.pdf), cication: [**-1**](None) 

	 *Inclusion AI, :, Bowen Ma, ..., Zizheng Yang, Zhengyu He*
- **OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding 
  LLM**, `arXiv, 2510.15870`, [arxiv](http://arxiv.org/abs/2510.15870v2), [pdf](http://arxiv.org/pdf/2510.15870v2.pdf), cication: [**-1**](None) 

	 *Hanrong Ye, Chao-Han Huck Yang, Arushi Goel, ..., Hongxu Yin, Pavlo Molchanov*
- **Qwen3-Omni Technical Report**, `arXiv, 2509.17765`, [arxiv](http://arxiv.org/abs/2509.17765v1), [pdf](http://arxiv.org/pdf/2509.17765v1.pdf), cication: [**-1**](None) 

	 *Jin Xu, Zhifang Guo, Hangrui Hu, ..., Jingren Zhou, Junyang Lin*


## LLM Tutorial
- ðŸŒŸ [The Smol Training Playbook:](https://huggingface.co/spaces/HuggingFaceTB/smol-playbook-toc)  ðŸ¤— 
- [**modded-nanogpt**](https://github.com/KellerJordan/modded-nanogpt) - KellerJordan ![Star](https://img.shields.io/github/stars/KellerJordan/modded-nanogpt.svg?style=social&label=Star) 
- [**nanoGPT**](https://github.com/karpathy/nanoGPT) - karpathy ![Star](https://img.shields.io/github/stars/karpathy/nanoGPT.svg?style=social&label=Star) 

## Open-source LLM
- **SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language
  Model**, `arXiv, 2502.02737`, [arxiv](http://arxiv.org/abs/2502.02737v1), [pdf](http://arxiv.org/pdf/2502.02737v1.pdf), cication: [**-1**](None) 

	 *Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, ..., Leandro von Werra, Thomas Wolf*