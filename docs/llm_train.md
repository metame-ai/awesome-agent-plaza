# LLM Training

## Survey


## LLM Training

- **Defeating the Training-Inference Mismatch via FP16**, `arXiv, 2510.26788`, [arxiv](http://arxiv.org/abs/2510.26788v1), [pdf](http://arxiv.org/pdf/2510.26788v1.pdf), cication: [**-1**](None) 

	 *Penghui Qi, Zichen Liu, Xiangxin Zhou, ..., Wee Sun Lee, Min Lin*

## FP8

- [FP8混合线性训练，MFU起飞金手指](https://zhuanlan.zhihu.com/p/1956374947229304351) 

## Tutorial
- [Post-training 101](https://tokens-for-thoughts.notion.site/post-training-101) 
## Toolkits
- [**torchforge**](https://github.com/meta-pytorch/torchforge) - meta-pytorch ![Star](https://img.shields.io/github/stars/meta-pytorch/torchforge.svg?style=social&label=Star)
- [**MONAI**](https://github.com/Project-MONAI/MONAI) - Project-MONAI ![Star](https://img.shields.io/github/stars/Project-MONAI/MONAI.svg?style=social&label=Star)
## Pretraining
- **Reinforcement Learning on Pre-Training Data**, `arXiv, 2509.19249`, [arxiv](http://arxiv.org/abs/2509.19249v2), [pdf](http://arxiv.org/pdf/2509.19249v2.pdf), cication: [**-1**](None) 

	 *Siheng Li, Kejiao Li, Zenan Xu, ..., Bo Zhou, Di Wang*